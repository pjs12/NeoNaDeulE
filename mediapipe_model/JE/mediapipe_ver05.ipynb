{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bcce0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time, os\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "from google.protobuf.json_format import MessageToDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75c22c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('./dataset_1/label.csv')\n",
    "labels = labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf841e74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "개 1 (75, 20, 94)\n",
      "공원 2 (112, 20, 94)\n",
      "금요일 3 (156, 20, 94)\n",
      "내년 4 (187, 20, 94)\n",
      "내일 5 (209, 20, 94)\n",
      "냄새나다 6 (236, 20, 94)\n",
      "누나 7 (262, 20, 94)\n",
      "동생 8 (292, 20, 94)\n",
      "목요일 9 (321, 20, 94)\n",
      "아래 10 (358, 20, 94)\n",
      "바다 11 (402, 20, 94)\n",
      "배고프다 12 (448, 20, 94)\n",
      "병원 13 (476, 20, 94)\n",
      "불 14 (518, 20, 94)\n",
      "산 15 (551, 20, 94)\n",
      "삼키다 16 (577, 20, 94)\n",
      "선생님 17 (615, 20, 94)\n",
      "수요일 18 (637, 20, 94)\n",
      "아빠 19 (671, 20, 94)\n",
      "아파트 20 (715, 20, 94)\n",
      "앞 21 (740, 20, 94)\n",
      "어제 22 (760, 20, 94)\n",
      "어지러움 23 (788, 20, 94)\n",
      "언니 24 (818, 20, 94)\n",
      "엄마 25 (837, 20, 94)\n",
      "오늘 26 (859, 20, 94)\n",
      "오른쪽 27 (895, 20, 94)\n",
      "오빠 28 (921, 20, 94)\n",
      "올해 29 (961, 20, 94)\n",
      "왼쪽 30 (1008, 20, 94)\n",
      "월요일 31 (1028, 20, 94)\n",
      "위에 32 (1073, 20, 94)\n",
      "음식물 33 (1121, 20, 94)\n",
      "일요일 34 (1174, 20, 94)\n",
      "자동차 35 (1223, 20, 94)\n",
      "작년 36 (1273, 20, 94)\n",
      "집 37 (1297, 20, 94)\n",
      "택시 38 (1339, 20, 94)\n",
      "토요일 39 (1379, 20, 94)\n",
      "학교 40 (1406, 20, 94)\n",
      "형 41 (1424, 20, 94)\n",
      "화요일 42 (1452, 20, 94)\n",
      "화장실 43 (1473, 20, 94)\n",
      "0 44 (1514, 20, 94)\n",
      "1 45 (1563, 20, 94)\n",
      "2 46 (1611, 20, 94)\n",
      "3 47 (1657, 20, 94)\n",
      "4 48 (1705, 20, 94)\n",
      "5 49 (1754, 20, 94)\n",
      "6 50 (1797, 20, 94)\n",
      "7 51 (1848, 20, 94)\n",
      "8 52 (1898, 20, 94)\n",
      "9 53 (1933, 20, 94)\n",
      "10 54 (1975, 20, 94)\n",
      "가렵다 55 (2034, 20, 94)\n",
      "개 56 (2083, 20, 94)\n",
      "공원 57 (2137, 20, 94)\n",
      "금요일 58 (2191, 20, 94)\n",
      "내년 59 (2225, 20, 94)\n",
      "내일 60 (2251, 20, 94)\n",
      "냄새나다 61 (2296, 20, 94)\n",
      "동생 62 (2319, 20, 94)\n",
      "목요일 63 (2363, 20, 94)\n",
      "물 64 (2405, 20, 94)\n",
      "바다 65 (2469, 20, 94)\n",
      "배고프다 66 (2526, 20, 94)\n",
      "불 67 (2578, 20, 94)\n",
      "삼키다 68 (2636, 20, 94)\n",
      "선생님 69 (2677, 20, 94)\n",
      "수요일 70 (2717, 20, 94)\n",
      "아빠 71 (2750, 20, 94)\n",
      "앞 72 (2781, 20, 94)\n",
      "어제 73 (2829, 20, 94)\n",
      "어지러움 74 (2878, 20, 94)\n",
      "누나 75 (2918, 20, 94)\n",
      "언니 76 (2958, 20, 94)\n",
      "엄마 77 (2992, 20, 94)\n",
      "오늘 78 (3018, 20, 94)\n",
      "오른쪽 79 (3066, 20, 94)\n",
      "오빠 80 (3097, 20, 94)\n",
      "올해 81 (3143, 20, 94)\n",
      "왼쪽 82 (3188, 20, 94)\n",
      "월요일 83 (3228, 20, 94)\n",
      "위에 84 (3269, 20, 94)\n",
      "음식물 85 (3318, 20, 94)\n",
      "일요일 86 (3378, 20, 94)\n",
      "자동차 87 (3439, 20, 94)\n",
      "작년 88 (3484, 20, 94)\n",
      "집 89 (3513, 20, 94)\n",
      "택시 90 (3557, 20, 94)\n",
      "토요일 91 (3607, 20, 94)\n",
      "학교 92 (3643, 20, 94)\n",
      "형 93 (3676, 20, 94)\n",
      "화요일 94 (3725, 20, 94)\n",
      "화장실 95 (3771, 20, 94)\n",
      "산 96 (3799, 20, 94)\n",
      "아래 97 (3826, 20, 94)\n",
      "0 98 (3854, 20, 94)\n",
      "1 99 (3888, 20, 94)\n",
      "2 100 (3921, 20, 94)\n",
      "3 101 (3961, 20, 94)\n",
      "4 102 (4002, 20, 94)\n",
      "5 103 (4040, 20, 94)\n",
      "6 104 (4073, 20, 94)\n",
      "7 105 (4115, 20, 94)\n",
      "8 106 (4156, 20, 94)\n",
      "9 107 (4179, 20, 94)\n",
      "10 108 (4209, 20, 94)\n",
      "가렵다 109 (4257, 20, 94)\n",
      "개 110 (4294, 20, 94)\n",
      "공원 111 (4337, 20, 94)\n",
      "금요일 112 (4383, 20, 94)\n",
      "내년 113 (4402, 20, 94)\n",
      "내일 114 (4436, 20, 94)\n",
      "냄새나다 115 (4480, 20, 94)\n",
      "누나 116 (4497, 20, 94)\n",
      "동생 117 (4504, 20, 94)\n",
      "목요일 118 (4543, 20, 94)\n",
      "물 119 (4574, 20, 94)\n",
      "바다 120 (4612, 20, 94)\n",
      "배고프다 121 (4664, 20, 94)\n",
      "불 122 (4708, 20, 94)\n",
      "삼키다 123 (4757, 20, 94)\n",
      "수요일 124 (4787, 20, 94)\n",
      "아빠 125 (4812, 20, 94)\n",
      "앞 126 (4835, 20, 94)\n",
      "어제 127 (4871, 20, 94)\n",
      "어지러움 128 (4904, 20, 94)\n",
      "언니 129 (4930, 20, 94)\n",
      "엄마 130 (4948, 20, 94)\n",
      "오늘 131 (4964, 20, 94)\n",
      "오른쪽 132 (5004, 20, 94)\n",
      "오빠 133 (5027, 20, 94)\n",
      "올해 134 (5068, 20, 94)\n",
      "왼쪽 135 (5111, 20, 94)\n",
      "월요일 136 (5143, 20, 94)\n",
      "위에 137 (5184, 20, 94)\n",
      "음식물 138 (5226, 20, 94)\n",
      "일요일 139 (5275, 20, 94)\n",
      "자동차 140 (5333, 20, 94)\n",
      "작년 141 (5369, 20, 94)\n",
      "집 142 (5386, 20, 94)\n",
      "택시 143 (5426, 20, 94)\n",
      "토요일 144 (5467, 20, 94)\n",
      "학교 145 (5499, 20, 94)\n",
      "형 146 (5523, 20, 94)\n",
      "화요일 147 (5561, 20, 94)\n",
      "화장실 148 (5595, 20, 94)\n",
      "산 149 (5616, 20, 94)\n",
      "아래 150 (5640, 20, 94)\n",
      "0 151 (5700, 20, 94)\n",
      "1 152 (5780, 20, 94)\n",
      "2 153 (5840, 20, 94)\n",
      "3 154 (5910, 20, 94)\n",
      "4 155 (5972, 20, 94)\n",
      "5 156 (6041, 20, 94)\n",
      "6 157 (6114, 20, 94)\n",
      "7 158 (6191, 20, 94)\n",
      "8 159 (6274, 20, 94)\n",
      "9 160 (6335, 20, 94)\n",
      "10 161 (6406, 20, 94)\n",
      "가렵다 162 (6462, 20, 94)\n",
      "개 163 (6522, 20, 94)\n",
      "공원 164 (6572, 20, 94)\n",
      "금요일 165 (6613, 20, 94)\n",
      "내년 166 (6666, 20, 94)\n",
      "내일 167 (6706, 20, 94)\n",
      "냄새나다 168 (6767, 20, 94)\n",
      "누나 169 (6802, 20, 94)\n",
      "동생 170 (6844, 20, 94)\n",
      "목요일 171 (6880, 20, 94)\n",
      "물 172 (6913, 20, 94)\n",
      "아래 173 (6960, 20, 94)\n",
      "바다 174 (7013, 20, 94)\n",
      "배고프다 175 (7064, 20, 94)\n",
      "불 176 (7111, 20, 94)\n",
      "산 177 (7145, 20, 94)\n",
      "삼키다 178 (7178, 20, 94)\n",
      "수요일 179 (7232, 20, 94)\n",
      "아빠 180 (7284, 20, 94)\n",
      "앞 181 (7321, 20, 94)\n",
      "어제 182 (7365, 20, 94)\n",
      "어지러움 183 (7408, 20, 94)\n",
      "언니 184 (7435, 20, 94)\n",
      "엄마 185 (7481, 20, 94)\n",
      "오늘 186 (7520, 20, 94)\n",
      "오른쪽 187 (7584, 20, 94)\n",
      "오빠 188 (7629, 20, 94)\n",
      "올해 189 (7692, 20, 94)\n",
      "왼쪽 190 (7760, 20, 94)\n",
      "월요일 191 (7805, 20, 94)\n",
      "위에 192 (7863, 20, 94)\n",
      "음식물 193 (7923, 20, 94)\n",
      "일요일 194 (7975, 20, 94)\n",
      "자동차 195 (8033, 20, 94)\n",
      "작년 196 (8082, 20, 94)\n",
      "집 197 (8121, 20, 94)\n",
      "친구 198 (8159, 20, 94)\n",
      "택시 199 (8211, 20, 94)\n",
      "토요일 200 (8254, 20, 94)\n",
      "학교 201 (8295, 20, 94)\n",
      "형 202 (8329, 20, 94)\n",
      "화요일 203 (8373, 20, 94)\n",
      "화장실 204 (8421, 20, 94)\n",
      "0 205 (8553, 20, 94)\n",
      "1 206 (8649, 20, 94)\n",
      "2 207 (8725, 20, 94)\n",
      "3 208 (8821, 20, 94)\n",
      "4 209 (8903, 20, 94)\n",
      "5 210 (8981, 20, 94)\n",
      "6 211 (9074, 20, 94)\n",
      "7 212 (9200, 20, 94)\n",
      "8 213 (9307, 20, 94)\n",
      "9 214 (9463, 20, 94)\n",
      "10 215 (9590, 20, 94)\n",
      "가렵다 216 (9657, 20, 94)\n",
      "개 217 (9717, 20, 94)\n",
      "공원 218 (9783, 20, 94)\n",
      "냄새나다 219 (9849, 20, 94)\n",
      "금요일 220 (9922, 20, 94)\n",
      "내년 221 (10017, 20, 94)\n",
      "누나 222 (10082, 20, 94)\n",
      "내일 223 (10160, 20, 94)\n",
      "동생 224 (10262, 20, 94)\n",
      "물 225 (10364, 20, 94)\n",
      "바다 226 (10429, 20, 94)\n",
      "목요일 227 (10503, 20, 94)\n",
      "아래 228 (10582, 20, 94)\n",
      "배고프다 229 (10675, 20, 94)\n",
      "병원 230 (10760, 20, 94)\n",
      "불 231 (10816, 20, 94)\n",
      "산 232 (10859, 20, 94)\n",
      "선생님 233 (10916, 20, 94)\n",
      "수요일 234 (11017, 20, 94)\n",
      "아빠 235 (11147, 20, 94)\n",
      "앞 236 (11211, 20, 94)\n",
      "어지러움 237 (11284, 20, 94)\n",
      "언니 238 (11341, 20, 94)\n",
      "엄마 239 (11412, 20, 94)\n",
      "오늘 240 (11486, 20, 94)\n",
      "오른쪽 241 (11568, 20, 94)\n",
      "오빠 242 (11660, 20, 94)\n",
      "올해 243 (11736, 20, 94)\n",
      "왼쪽 244 (11818, 20, 94)\n",
      "월요일 245 (11908, 20, 94)\n",
      "위에 246 (11980, 20, 94)\n",
      "음식물 247 (12058, 20, 94)\n",
      "일요일 248 (12135, 20, 94)\n",
      "자동차 249 (12198, 20, 94)\n",
      "작년 250 (12267, 20, 94)\n",
      "집 251 (12350, 20, 94)\n",
      "친구 252 (12412, 20, 94)\n",
      "택시 253 (12492, 20, 94)\n",
      "토요일 254 (12570, 20, 94)\n",
      "학교 255 (12633, 20, 94)\n",
      "형 256 (12698, 20, 94)\n",
      "화요일 257 (12782, 20, 94)\n",
      "화장실 258 (12907, 20, 94)\n",
      "0 259 (12950, 20, 94)\n",
      "1 260 (12988, 20, 94)\n",
      "2 261 (13031, 20, 94)\n",
      "3 262 (13068, 20, 94)\n",
      "4 263 (13098, 20, 94)\n",
      "5 264 (13146, 20, 94)\n",
      "6 265 (13178, 20, 94)\n",
      "7 266 (13222, 20, 94)\n",
      "8 267 (13262, 20, 94)\n",
      "9 268 (13309, 20, 94)\n",
      "10 269 (13373, 20, 94)\n",
      "가렵다 270 (13441, 20, 94)\n",
      "개 271 (13517, 20, 94)\n",
      "공원 272 (13591, 20, 94)\n",
      "금요일 273 (13641, 20, 94)\n",
      "내년 274 (13702, 20, 94)\n",
      "내일 275 (13755, 20, 94)\n",
      "냄새나다 276 (13792, 20, 94)\n",
      "누나 277 (13836, 20, 94)\n",
      "동생 278 (13882, 20, 94)\n",
      "목요일 279 (13936, 20, 94)\n",
      "물 280 (13990, 20, 94)\n",
      "아래 281 (14061, 20, 94)\n",
      "바다 282 (14122, 20, 94)\n",
      "배고프다 283 (14186, 20, 94)\n",
      "불 284 (14252, 20, 94)\n",
      "산 285 (14301, 20, 94)\n",
      "삼키다 286 (14352, 20, 94)\n",
      "선생님 287 (14403, 20, 94)\n",
      "수요일 288 (14438, 20, 94)\n",
      "아빠 289 (14485, 20, 94)\n",
      "앞 290 (14526, 20, 94)\n",
      "어제 291 (14561, 20, 94)\n",
      "어지러움 292 (14602, 20, 94)\n",
      "언니 293 (14636, 20, 94)\n",
      "엄마 294 (14682, 20, 94)\n",
      "오늘 295 (14722, 20, 94)\n",
      "오른쪽 296 (14780, 20, 94)\n",
      "오빠 297 (14819, 20, 94)\n",
      "올해 298 (14880, 20, 94)\n",
      "왼쪽 299 (14935, 20, 94)\n",
      "월요일 300 (14967, 20, 94)\n",
      "위에 301 (15015, 20, 94)\n",
      "음식물 302 (15073, 20, 94)\n",
      "일요일 303 (15121, 20, 94)\n",
      "자동차 304 (15171, 20, 94)\n",
      "작년 305 (15216, 20, 94)\n",
      "집 306 (15261, 20, 94)\n",
      "친구 307 (15305, 20, 94)\n",
      "택시 308 (15358, 20, 94)\n",
      "토요일 309 (15411, 20, 94)\n",
      "학교 310 (15466, 20, 94)\n",
      "형 311 (15505, 20, 94)\n",
      "화요일 312 (15534, 20, 94)\n",
      "화장실 313 (15569, 20, 94)\n",
      "0 314 (15622, 20, 94)\n",
      "1 315 (15667, 20, 94)\n",
      "2 316 (15712, 20, 94)\n",
      "3 317 (15754, 20, 94)\n",
      "4 318 (15787, 20, 94)\n",
      "5 319 (15836, 20, 94)\n",
      "8 320 (15886, 20, 94)\n",
      "9 321 (15937, 20, 94)\n",
      "10 322 (16005, 20, 94)\n",
      "가렵다 323 (16076, 20, 94)\n",
      "개 324 (16159, 20, 94)\n",
      "공원 325 (16240, 20, 94)\n",
      "금요일 326 (16290, 20, 94)\n",
      "내년 327 (16354, 20, 94)\n",
      "내일 328 (16413, 20, 94)\n",
      "냄새나다 329 (16463, 20, 94)\n",
      "누나 330 (16514, 20, 94)\n",
      "동생 331 (16558, 20, 94)\n",
      "목요일 332 (16612, 20, 94)\n",
      "물 333 (16664, 20, 94)\n",
      "아래 334 (16732, 20, 94)\n",
      "바다 335 (16766, 20, 94)\n",
      "배고프다 336 (16828, 20, 94)\n",
      "불 337 (16888, 20, 94)\n",
      "산 338 (16935, 20, 94)\n",
      "삼키다 339 (16981, 20, 94)\n",
      "선생님 340 (17031, 20, 94)\n",
      "수요일 341 (17065, 20, 94)\n",
      "아빠 342 (17107, 20, 94)\n",
      "앞 343 (17141, 20, 94)\n",
      "어제 344 (17172, 20, 94)\n",
      "어지러움 345 (17206, 20, 94)\n",
      "언니 346 (17235, 20, 94)\n",
      "엄마 347 (17273, 20, 94)\n",
      "오늘 348 (17311, 20, 94)\n",
      "오른쪽 349 (17365, 20, 94)\n",
      "오빠 350 (17398, 20, 94)\n",
      "올해 351 (17453, 20, 94)\n",
      "왼쪽 352 (17502, 20, 94)\n",
      "월요일 353 (17521, 20, 94)\n",
      "위에 354 (17565, 20, 94)\n",
      "음식물 355 (17618, 20, 94)\n",
      "일요일 356 (17663, 20, 94)\n",
      "자동차 357 (17716, 20, 94)\n",
      "작년 358 (17759, 20, 94)\n",
      "집 359 (17799, 20, 94)\n",
      "친구 360 (17850, 20, 94)\n",
      "택시 361 (17906, 20, 94)\n",
      "토요일 362 (17958, 20, 94)\n",
      "학교 363 (18006, 20, 94)\n",
      "형 364 (18040, 20, 94)\n",
      "화요일 365 (18067, 20, 94)\n",
      "화장실 366 (18098, 20, 94)\n",
      "6 367 (18135, 20, 94)\n",
      "7 368 (18172, 20, 94)\n",
      "0 369 (18190, 20, 94)\n",
      "1 370 (18214, 20, 94)\n",
      "2 371 (18241, 20, 94)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 372 (18273, 20, 94)\n",
      "4 373 (18299, 20, 94)\n",
      "5 374 (18328, 20, 94)\n",
      "6 375 (18352, 20, 94)\n",
      "7 376 (18379, 20, 94)\n",
      "8 377 (18402, 20, 94)\n",
      "9 378 (18424, 20, 94)\n",
      "10 379 (18453, 20, 94)\n",
      "가렵다 380 (18472, 20, 94)\n",
      "개 381 (18513, 20, 94)\n"
     ]
    }
   ],
   "source": [
    "actions = labels\n",
    "# LSTM Window Size\n",
    "seq_length = 20\n",
    "\n",
    "VIDEO_FILES = []\n",
    "dir_path = './dataset_1/viedo'\n",
    "for (root, directories, files) in os.walk(dir_path):\n",
    "    for file in files:\n",
    "        VIDEO_FILES.append(os.path.join(root, file))\n",
    "\n",
    "# MediaPipe hands model\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(\n",
    "    model_complexity=0,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5)\n",
    "\n",
    "os.makedirs('dataset', exist_ok=True)\n",
    "zero = np.zeros(15) \n",
    "\n",
    "for idx, file in enumerate(VIDEO_FILES):\n",
    "    cap = cv2.VideoCapture(file)\n",
    "    action = actions[idx][0]\n",
    "    data = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "\n",
    "        ret, img = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        img = cv2.resize(img, dsize=(800,450))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        result = hands.process(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if result.multi_hand_landmarks:\n",
    "            data_arr = []\n",
    "            right_hand, left_hand = np.zeros((21,3)), np.zeros((21,3))\n",
    "            for res in result.multi_hand_landmarks:\n",
    "                joint = np.zeros((21,3))\n",
    "                for j, lm in enumerate(res.landmark):\n",
    "                    joint[j] = [lm.x, lm.y, lm.z]\n",
    "\n",
    "                # Compute angles between joints\n",
    "                v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :3] # Parent joint\n",
    "                v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :3] # Child joint\n",
    "                v = v2 - v1 # [20, 3]\n",
    "                # Normalize v\n",
    "                v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]\n",
    "\n",
    "                # Get angle using arcos of dot product\n",
    "                angle = np.arccos(np.einsum('nt,nt->n',\n",
    "                    v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:], \n",
    "                    v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:])) # [15,]\n",
    "\n",
    "                angle = np.degrees(angle) # Convert radian to degree\n",
    "\n",
    "                angle_label = np.array(angle, dtype=np.float32)\n",
    "                # angle_label = np.append(angle_label, idx)\n",
    "                handedness_dict = MessageToDict(result.multi_handedness[0])\n",
    "                if handedness_dict['classification'][0]['label'] == 'Right':\n",
    "                    right_hand = joint\n",
    "                else:\n",
    "                    left_hand = joint\n",
    "                    \n",
    "\n",
    "                mp_drawing.draw_landmarks(img, res, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "                data_arr.extend(angle_label)\n",
    "               \n",
    "            if len(data_arr) == 15:\n",
    "                handedness_dict = MessageToDict(result.multi_handedness[0])\n",
    "                if handedness_dict['classification'][0]['label'] == 'Right':\n",
    "                    data_arr = np.concatenate((zero, data_arr))\n",
    "                else:\n",
    "                    data_arr = np.concatenate((data_arr, zero))\n",
    "            elif len(data_arr) > 30:\n",
    "                continue\n",
    "                \n",
    "            hand_distance = left_hand - right_hand\n",
    "            hand_distance /= np.linalg.norm(hand_distance, axis=1)[:, np.newaxis]\n",
    "            data_arr = np.concatenate((data_arr, hand_distance.flatten()))\n",
    "            data_arr = np.append(data_arr, idx)\n",
    "            data.append(data_arr)\n",
    "            \n",
    "        cv2.imshow('img', img)\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "    \n",
    "    data = np.array(data)\n",
    "\n",
    "    try:\n",
    "        full_seq_data = []\n",
    "        for seq in range(len(data) - seq_length):\n",
    "            full_seq_data.append(data[seq:seq + seq_length])\n",
    "        full_seq_data = np.array(full_seq_data)\n",
    "\n",
    "        if idx == 0:\n",
    "            full_data = full_seq_data\n",
    "            continue\n",
    "\n",
    "        full_data = np.concatenate((full_data, full_seq_data))\n",
    "        print(action,idx, full_data.shape)\n",
    "        \n",
    "    except:\n",
    "        print('ERROR!!!!!!!!!!!!!!!!!', action, idx)\n",
    "        pass\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n",
    "\n",
    "np.save(os.path.join('dataset/fulldata_angle_distance'), full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38ce85c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./dataset/fulldata_angle_distance.npy')\n",
    "labels = pd.read_csv('./dataset_1/label.csv')\n",
    "lab = labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e0ca57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr= set()\n",
    "\n",
    "for i in lab:\n",
    "    arr.add(i[0])\n",
    "    \n",
    "    \n",
    "arr = list(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af8df29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    num = data[i,0,-1]\n",
    "    label_num = lab[int(num)][0]\n",
    "    num = arr.index(label_num)\n",
    "    data[i,:,-1] = num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28ce157b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18513, 20, 93), (18513,))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data[:,:,:-1]\n",
    "y = data[:,0,-1]\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d22e438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3e6c6d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75aabd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af3c0746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 128)               113664    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 88)                2904      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 143,416\n",
      "Trainable params: 143,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(20, 93)))\n",
    "model.add(Dense(128, activation='tanh'))\n",
    "model.add(Dense(64, activation='tanh'))\n",
    "model.add(Dense(32, activation='tanh'))\n",
    "model.add(Dense(y.shape[-1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84544858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'04-26_18:21'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "t = datetime.datetime.now()\n",
    "t = str(t)[5:10] +'_' +str(t)[11:16]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "256ce096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "370/371 [============================>.] - ETA: 0s - loss: 2.7127 - accuracy: 0.4347\n",
      "Epoch 1: val_loss improved from inf to 1.76335, saving model to model_04-26_18:21.h5\n",
      "371/371 [==============================] - 8s 18ms/step - loss: 2.7119 - accuracy: 0.4349 - val_loss: 1.7634 - val_accuracy: 0.6097\n",
      "Epoch 2/100\n",
      "370/371 [============================>.] - ETA: 0s - loss: 1.4080 - accuracy: 0.6762\n",
      "Epoch 2: val_loss improved from 1.76335 to 1.19140, saving model to model_04-26_18:21.h5\n",
      "371/371 [==============================] - 6s 16ms/step - loss: 1.4080 - accuracy: 0.6761 - val_loss: 1.1914 - val_accuracy: 0.7083\n",
      "Epoch 3/100\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.9876 - accuracy: 0.7564\n",
      "Epoch 3: val_loss improved from 1.19140 to 0.88472, saving model to model_04-26_18:21.h5\n",
      "371/371 [==============================] - 6s 16ms/step - loss: 0.9876 - accuracy: 0.7564 - val_loss: 0.8847 - val_accuracy: 0.7782\n",
      "Epoch 4/100\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.7490 - accuracy: 0.8085\n",
      "Epoch 4: val_loss improved from 0.88472 to 0.67125, saving model to model_04-26_18:21.h5\n",
      "371/371 [==============================] - 6s 16ms/step - loss: 0.7490 - accuracy: 0.8085 - val_loss: 0.6713 - val_accuracy: 0.8265\n",
      "Epoch 5/100\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.5651 - accuracy: 0.8597\n",
      "Epoch 5: val_loss improved from 0.67125 to 0.57576, saving model to model_04-26_18:21.h5\n",
      "371/371 [==============================] - 6s 16ms/step - loss: 0.5651 - accuracy: 0.8597 - val_loss: 0.5758 - val_accuracy: 0.8471\n",
      "Epoch 6/100\n",
      "368/371 [============================>.] - ETA: 0s - loss: 0.4330 - accuracy: 0.8911\n",
      "Epoch 6: val_loss improved from 0.57576 to 0.46268, saving model to model_04-26_18:21.h5\n",
      "371/371 [==============================] - 6s 16ms/step - loss: 0.4336 - accuracy: 0.8908 - val_loss: 0.4627 - val_accuracy: 0.8828\n",
      "Epoch 7/100\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.3840 - accuracy: 0.8994\n",
      "Epoch 7: val_loss improved from 0.46268 to 0.39841, saving model to model_04-26_18:21.h5\n",
      "371/371 [==============================] - 6s 16ms/step - loss: 0.3840 - accuracy: 0.8994 - val_loss: 0.3984 - val_accuracy: 0.8832\n",
      "Epoch 8/100\n",
      "370/371 [============================>.] - ETA: 0s - loss: 0.3121 - accuracy: 0.9178\n",
      "Epoch 8: val_loss improved from 0.39841 to 0.34291, saving model to model_04-26_18:21.h5\n",
      "371/371 [==============================] - 6s 17ms/step - loss: 0.3123 - accuracy: 0.9178 - val_loss: 0.3429 - val_accuracy: 0.9045\n",
      "Epoch 9/100\n",
      "369/371 [============================>.] - ETA: 0s - loss: 0.2733 - accuracy: 0.9248\n",
      "Epoch 9: val_loss improved from 0.34291 to 0.30966, saving model to model_04-26_18:21.h5\n",
      "371/371 [==============================] - 7s 19ms/step - loss: 0.2730 - accuracy: 0.9251 - val_loss: 0.3097 - val_accuracy: 0.9180\n",
      "Epoch 10/100\n",
      "370/371 [============================>.] - ETA: 0s - loss: 0.2759 - accuracy: 0.9246\n",
      "Epoch 10: val_loss improved from 0.30966 to 0.24896, saving model to model_04-26_18:21.h5\n",
      "371/371 [==============================] - 7s 18ms/step - loss: 0.2757 - accuracy: 0.9246 - val_loss: 0.2490 - val_accuracy: 0.9311\n",
      "Epoch 11/100\n",
      "370/371 [============================>.] - ETA: 0s - loss: 0.2040 - accuracy: 0.9436\n",
      "Epoch 11: val_loss improved from 0.24896 to 0.23698, saving model to model_04-26_18:21.h5\n",
      "371/371 [==============================] - 7s 18ms/step - loss: 0.2040 - accuracy: 0.9436 - val_loss: 0.2370 - val_accuracy: 0.9365\n",
      "Epoch 12/100\n",
      "369/371 [============================>.] - ETA: 0s - loss: 0.2071 - accuracy: 0.9411\n",
      "Epoch 12: val_loss did not improve from 0.23698\n",
      "371/371 [==============================] - 7s 18ms/step - loss: 0.2071 - accuracy: 0.9412 - val_loss: 0.2868 - val_accuracy: 0.9149\n",
      "Epoch 13/100\n",
      "369/371 [============================>.] - ETA: 0s - loss: 0.2608 - accuracy: 0.9247\n",
      "Epoch 13: val_loss did not improve from 0.23698\n",
      "371/371 [==============================] - 7s 18ms/step - loss: 0.2611 - accuracy: 0.9245 - val_loss: 0.3254 - val_accuracy: 0.8967\n",
      "Epoch 14/100\n",
      "369/371 [============================>.] - ETA: 0s - loss: 0.2170 - accuracy: 0.9375\n",
      "Epoch 14: val_loss did not improve from 0.23698\n",
      "371/371 [==============================] - 7s 19ms/step - loss: 0.2169 - accuracy: 0.9375 - val_loss: 0.2655 - val_accuracy: 0.9173\n",
      "Epoch 15/100\n",
      "369/371 [============================>.] - ETA: 0s - loss: 0.2148 - accuracy: 0.9374\n",
      "Epoch 15: val_loss did not improve from 0.23698\n",
      "371/371 [==============================] - 7s 18ms/step - loss: 0.2147 - accuracy: 0.9374 - val_loss: 0.2433 - val_accuracy: 0.9254\n",
      "Epoch 16/100\n",
      "370/371 [============================>.] - ETA: 0s - loss: 0.1621 - accuracy: 0.9535\n",
      "Epoch 16: val_loss improved from 0.23698 to 0.21013, saving model to model_04-26_18:21.h5\n",
      "371/371 [==============================] - 7s 19ms/step - loss: 0.1620 - accuracy: 0.9536 - val_loss: 0.2101 - val_accuracy: 0.9325\n",
      "Epoch 17/100\n",
      "370/371 [============================>.] - ETA: 0s - loss: 0.1357 - accuracy: 0.9606\n",
      "Epoch 17: val_loss improved from 0.21013 to 0.18196, saving model to model_04-26_18:21.h5\n",
      "371/371 [==============================] - 7s 18ms/step - loss: 0.1357 - accuracy: 0.9607 - val_loss: 0.1820 - val_accuracy: 0.9426\n",
      "Epoch 18/100\n",
      "369/371 [============================>.] - ETA: 0s - loss: 0.1750 - accuracy: 0.9452\n",
      "Epoch 18: val_loss did not improve from 0.18196\n",
      "371/371 [==============================] - 7s 18ms/step - loss: 0.1749 - accuracy: 0.9451 - val_loss: 0.2711 - val_accuracy: 0.9115\n",
      "Epoch 19/100\n",
      "368/371 [============================>.] - ETA: 0s - loss: 0.2107 - accuracy: 0.9331\n",
      "Epoch 19: val_loss did not improve from 0.18196\n",
      "371/371 [==============================] - 7s 18ms/step - loss: 0.2105 - accuracy: 0.9332 - val_loss: 0.2355 - val_accuracy: 0.9264\n",
      "Epoch 20/100\n",
      "370/371 [============================>.] - ETA: 0s - loss: 0.1256 - accuracy: 0.9624\n",
      "Epoch 20: val_loss improved from 0.18196 to 0.13879, saving model to model_04-26_18:21.h5\n",
      "371/371 [==============================] - 7s 18ms/step - loss: 0.1255 - accuracy: 0.9624 - val_loss: 0.1388 - val_accuracy: 0.9568\n",
      "Epoch 21/100\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.1043 - accuracy: 0.9694\n",
      "Epoch 21: val_loss did not improve from 0.13879\n",
      "371/371 [==============================] - 7s 18ms/step - loss: 0.1043 - accuracy: 0.9694 - val_loss: 0.1768 - val_accuracy: 0.9433\n",
      "Epoch 22/100\n",
      "368/371 [============================>.] - ETA: 0s - loss: 0.0894 - accuracy: 0.9743\n",
      "Epoch 22: val_loss did not improve from 0.13879\n",
      "371/371 [==============================] - 7s 18ms/step - loss: 0.0894 - accuracy: 0.9742 - val_loss: 0.1602 - val_accuracy: 0.9433\n",
      "Epoch 23/100\n",
      "370/371 [============================>.] - ETA: 0s - loss: 0.1449 - accuracy: 0.9545\n",
      "Epoch 23: val_loss did not improve from 0.13879\n",
      "371/371 [==============================] - 7s 19ms/step - loss: 0.1449 - accuracy: 0.9545 - val_loss: 0.2226 - val_accuracy: 0.9291\n",
      "Epoch 24/100\n",
      "370/371 [============================>.] - ETA: 0s - loss: 0.2296 - accuracy: 0.9258\n",
      "Epoch 24: val_loss did not improve from 0.13879\n",
      "371/371 [==============================] - 7s 18ms/step - loss: 0.2297 - accuracy: 0.9258 - val_loss: 0.2990 - val_accuracy: 0.9058\n",
      "Epoch 25/100\n",
      "370/371 [============================>.] - ETA: 0s - loss: 0.1340 - accuracy: 0.9581Restoring model weights from the end of the best epoch: 20.\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.13879\n",
      "371/371 [==============================] - 7s 18ms/step - loss: 0.1341 - accuracy: 0.9581 - val_loss: 0.2000 - val_accuracy: 0.9409\n",
      "Epoch 25: early stopping\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1, min_delta=0)\n",
    "mc = ModelCheckpoint(f'model_{t}.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "\n",
    "hist = model.fit(x_train, y_train, validation_split=0.2, epochs=100, callbacks=[es, mc], verbose=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12b56669",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_0426_1821.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0aa06367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.957871995679179"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "88dc55a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "\n",
    "actions = arr\n",
    "seq_length = 20\n",
    "\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "seq = []\n",
    "action_seq = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, img = cap.read()\n",
    "    img0 = img.copy()\n",
    "\n",
    "    img = cv2.flip(img, 1)\n",
    "    img = cv2.resize(img, dsize=(800,450))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    if result.multi_hand_landmarks is not None:\n",
    "        hand_arr = []\n",
    "        right_hand, left_hand = np.zeros((21,3)), np.zeros((21,3))\n",
    "        for res in result.multi_hand_landmarks:\n",
    "            joint = np.zeros((21, 3))\n",
    "            for j, lm in enumerate(res.landmark):\n",
    "                joint[j] = [lm.x, lm.y, lm.z]\n",
    "\n",
    "            # Compute angles between joints\n",
    "            v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :3] # Parent joint\n",
    "            v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :3] # Child joint\n",
    "            v = v2 - v1 # [20, 3]\n",
    "            # Normalize v\n",
    "            v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]\n",
    "\n",
    "            # Get angle using arcos of dot product\n",
    "            angle = np.arccos(np.einsum('nt,nt->n',\n",
    "                v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:], \n",
    "                v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:])) # [15,]\n",
    "\n",
    "            angle = np.degrees(angle) # Convert radian to degree\n",
    "            angle_label = np.array(angle, dtype=np.float32)\n",
    "            \n",
    "            handedness_dict = MessageToDict(result.multi_handedness[0])\n",
    "            if handedness_dict['classification'][0]['label'] == 'Right':\n",
    "                right_hand = joint\n",
    "            else:\n",
    "                left_hand = joint\n",
    "\n",
    "            \n",
    "            hand_arr.extend(angle_label)\n",
    "            \n",
    "            mp_drawing.draw_landmarks(img, res, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "        if len(hand_arr) == 15:\n",
    "            handedness_dict = MessageToDict(result.multi_handedness[0])\n",
    "            if handedness_dict['classification'][0]['label'] == 'Right':\n",
    "                hand_arr = np.concatenate((np.zeros(15), hand_arr))\n",
    "            else:\n",
    "                hand_arr = np.concatenate((hand_arr, np.zeros(15)))\n",
    "        elif len(hand_arr) > 30:\n",
    "            continue\n",
    "            \n",
    "        hand_distance = left_hand - right_hand\n",
    "        hand_distance /= np.linalg.norm(hand_distance, axis=1)[:, np.newaxis]\n",
    "        hand_arr = np.concatenate((hand_arr, hand_distance.flatten()))\n",
    "        seq.append(hand_arr)\n",
    "        \n",
    "        if len(seq) < seq_length:\n",
    "            continue\n",
    "            \n",
    "        input_data = np.expand_dims(np.array(seq[-seq_length:], dtype=np.float32), axis=0)\n",
    "        y_pred = model.predict(input_data).squeeze()\n",
    "        i_pred = int(np.argmax(y_pred))\n",
    "        conf = y_pred[i_pred]\n",
    "        if conf < 0.7:\n",
    "            continue\n",
    "        \n",
    "        action = actions[i_pred]\n",
    "        action_seq.append(action)\n",
    "        if len(action_seq) < 3:\n",
    "            continue\n",
    "        this_action = '?'\n",
    "        if action_seq[-1] == action_seq[-2] == action_seq[-3]:\n",
    "            this_action = action\n",
    "        font = ImageFont.truetype(\"fonts/gulim.ttc\", 20)\n",
    "        img = Image.fromarray(img)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        draw.text((30,50), this_action, font=font, fill=(0,0,255))\n",
    "        img = np.array(img)\n",
    "\n",
    "    cv2.imshow('img', img)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "            \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca5a318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
